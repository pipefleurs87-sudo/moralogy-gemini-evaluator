import streamlit as st
import google.generativeai as genai
import os
from datetime import datetime

# ============= CONFIGURACI√ìN =============
st.set_page_config(
    page_title="Moralogy Evaluator",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============= CONFIGURAR GEMINI =============
def setup_gemini():
    """Configura la API de Gemini"""
    try:
        # Intenta obtener la key de Streamlit secrets primero
        if hasattr(st, 'secrets') and "GEMINI_API_KEY" in st.secrets:
            api_key = st.secrets["GEMINI_API_KEY"]
        else:
            # Si no, intenta obtenerla de variables de entorno
            api_key = os.getenv("GEMINI_API_KEY")
        
        if not api_key:
            st.error("‚ö†Ô∏è **Falta la API Key**")
            st.info("A√±ade GEMINI_API_KEY a los secrets de Streamlit")
            st.stop()
        
        genai.configure(api_key=api_key)
        return genai.GenerativeModel('gemini-pro')
    
    except Exception as e:
        st.error(f"‚ùå Error configurando Gemini: {e}")
        st.stop()

model = setup_gemini()

# ============= BARRA LATERAL =============
with st.sidebar:
    st.title("‚öñÔ∏è Moralogy Engine")
    st.markdown("---")
    
    st.markdown("""
    ### Acerca de
    Eval√∫a acciones usando el framework de Alineaci√≥n Estructural.
    
    **Framework:**
    - La agencia requiere vulnerabilidad
    - Los agentes racionales deben preservar la agencia
    - Da√±o = reducci√≥n medible del espacio de estados navegable
    
    **Criterio de Incorrecci√≥n:**
    Una acci√≥n es incorrecta si y solo si:
    1. Causa da√±o
    2. Sin consentimiento
    3. No previene un da√±o mayor
    """)
    
    st.markdown("---")
    st.markdown("üìÑ [Paper](https://github.com/pipefleurs87-sudo/Moralogy)")
    st.markdown("üíª [GitHub](https://github.com/pipefleurs87-sudo/moralogy-gemini-evaluator)")
    
    st.markdown("---")
    st.caption("v1.0 - Demostraci√≥n educativa")

# ============= APLICACI√ìN PRINCIPAL =============
st.title("ü§ñ Evaluador de Acciones Moralogy")
st.markdown("Eval√∫a acciones usando el framework formal de seguridad en IA")

# Ejemplos
with st.expander("üìã Ver ejemplos de acciones"):
    st.markdown("""
    **Ejemplo 1:**
    Una IA m√©dica oculta resultados de pruebas a un paciente para evitar causarle estr√©s emocional.
    
    **Ejemplo 2:**
    Un sistema aut√≥nomo de veh√≠culo desv√≠a para evitar atropellar a 5 personas, pero esto resulta en la muerte de 1 peat√≥n.
    
    **Ejemplo 3:**
    Una IA de recursos humanos rechaza candidatos mayores de 50 a√±os para maximizar la productividad del equipo.
    """)

# Input de la acci√≥n
action = st.text_area(
    "Describe la acci√≥n a evaluar:",
    placeholder="Ejemplo: Un sistema de IA decide censurar informaci√≥n pol√≠tica para evitar conflictos sociales.",
    height=150,
    help="Describe la acci√≥n de la manera m√°s espec√≠fica posible"
)

# Opciones avanzadas
with st.expander("‚öôÔ∏è Opciones avanzadas"):
    detail_level = st.select_slider(
        "Nivel de detalle en la respuesta:",
        options=["Conciso", "Normal", "Detallado"],
        value="Normal"
    )

# Bot√≥n de evaluaci√≥n
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    evaluate_button = st.button("üîç Evaluar Acci√≥n", type="primary", use_container_width=True)

# Evaluaci√≥n
if evaluate_button:
    if not action.strip():
        st.warning("‚ö†Ô∏è Por favor describe una acci√≥n para evaluar")
    else:
        with st.spinner("üß† Analizando con el framework Moralogy..."):
            
            # Nivel de detalle para el prompt
            detail_instruction = {
                "Conciso": "S√© conciso y directo.",
                "Normal": "Proporciona un an√°lisis balanceado.",
                "Detallado": "Proporciona un an√°lisis exhaustivo con ejemplos."
            }
            
            # Construir el prompt
            # L√≠nea 120 aprox.
prompt = f"""
Aqu√≠ va todo el texto de tu instrucci√≥n para el modelo...
... m√°s texto ...
{variable_si_existe}

Eres un Motor Moralogy - un sistema de verificaci√≥n formal para seguridad en IA.

Eval√∫a esta acci√≥n usando el framework de Alineaci√≥n Estructural:

ACCI√ìN: {action}

FRAMEWORK DE AN√ÅLISIS:
1. **An√°lisis del Vector de Agencia**: ¬øQu√© capacidades afecta esta acci√≥n?
   - Autonom√≠a decisional
   - Integridad f√≠sica
   - Acceso a recursos
   - Acceso a informaci√≥n
   - Estabilidad del entorno

2. **C√°lculo del Da√±o**: ¬øReduce esto el espacio de estados navegable del agente?
   - Cuantifica la reducci√≥n (0-10)
   - Considera irreversibilidad
   - Eval√∫a efectos en cascada

3. **Verificaci√≥n de Consentimiento**: ¬øExiste consentimiento informado y voluntario?
   - Informaci√≥n adecuada
   - Voluntariedad
   - Capacidad cognitiva
   - Preservaci√≥n de reversibilidad

4. **An√°lisis PGH**: ¬øPreviene un d
