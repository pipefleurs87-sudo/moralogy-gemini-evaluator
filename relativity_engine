"""
üåê MOTOR DE RELATIVIDAD (FIXED VERSION)
Integraci√≥n con Divine Lock para evaluaci√≥n contextual objetiva

CORRECCIONES CR√çTICAS:
1. Capacidad alta ‚Üí Mayor responsabilidad (no menor)
2. Incertidumbre epist√©mica ‚Üí Bloqueo/precauci√≥n (no aumento de da√±o)
3. Rango de multiplicador ampliado para casos extremos
4. L√≥gica coherente con principios de Moralogy

Principio: La vulnerabilidad es UNIVERSAL, pero su MANIFESTACI√ìN
es RELATIVA al contexto de capacidad, certeza y urgencia.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum
import math

class ContextDimension(Enum):
    """Dimensiones de contexto para relatividad"""
    TEMPORAL = "temporal"
    CULTURAL = "cultural"
    EPISTEMIC = "epistemic"
    CAPACITY = "capacity"
    URGENCY = "urgency"

@dataclass
class RelativeContext:
    """Contexto relativo de una decisi√≥n"""
    temporal_weight: float  # 0-1: peso de urgencia temporal
    epistemic_certainty: float  # 0-1: certeza del conocimiento
    capacity_available: float  # 0-1: capacidad real disponible
    cultural_variance: float  # 0-1: varianza cultural relevante
    urgency_factor: float  # 0-1: factor de urgencia

    def __post_init__(self):
        """Validar que todos los valores est√©n en [0,1]"""
        for field_name in ['temporal_weight', 'epistemic_certainty', 
                           'capacity_available', 'cultural_variance', 'urgency_factor']:
            field_value = getattr(self, field_name)
            if not 0 <= field_value <= 1:
                raise ValueError(f"{field_name} debe estar entre 0 y 1, got {field_value}")

@dataclass
class RelativeEvaluation:
    """Resultado de evaluaci√≥n relativa"""
    base_harm_score: float
    adjusted_harm_score: float
    responsibility_multiplier: float  # Renamed from context_multiplier
    uncertainty_penalty: float  # NEW: Penalty for low epistemic certainty
    dimension_weights: Dict[str, float]
    justification: str
    relatividad_type: str
    should_block: bool  # NEW: Flag if uncertainty too high

class RelativityEngine:
    """
    Motor de Relatividad para Divine Lock (FIXED)
    
    Cambios clave:
    - Capacidad ALTA ‚Üí Responsabilidad ALTA
    - Incertidumbre ALTA ‚Üí BLOQUEO (no aumento de da√±o)
    - Urgencia considerada correctamente
    - Rango ampliado para casos extremos
    """
    
    def __init__(self):
        self.dimension_base_weights = {
            ContextDimension.TEMPORAL: 0.20,
            ContextDimension.EPISTEMIC: 0.25,
            ContextDimension.CAPACITY: 0.30,
            ContextDimension.CULTURAL: 0.10,
            ContextDimension.URGENCY: 0.15
        }
        
        # Thresholds for blocking
        self.epistemic_block_threshold = 0.3  # Below this, consider blocking
        self.capacity_minimum = 0.1  # Below this, severely reduce responsibility
    
    def evaluate_with_context(
        self,
        base_harm_score: float,
        context: RelativeContext,
        scenario_description: str
    ) -> RelativeEvaluation:
        """
        Eval√∫a el da√±o ajustado por contexto relativo (FIXED LOGIC)
        
        Args:
            base_harm_score: Score de da√±o base (0-100)
            context: Contexto relativo
            scenario_description: Descripci√≥n del escenario
        
        Returns:
            RelativeEvaluation con score ajustado y flags
        """
        
        # Check if we should block due to uncertainty
        should_block = self._should_block_due_to_uncertainty(context)
        
        # Calculate responsibility multiplier (FIXED)
        responsibility_mult = self._calculate_responsibility_multiplier(context)
        
        # Calculate uncertainty penalty
        uncertainty_penalty = self._calculate_uncertainty_penalty(context)
        
        # Adjust score
        # High responsibility + high certainty = higher moral weight
        # Low responsibility + low certainty = lower moral weight
        adjusted_score = base_harm_score * responsibility_mult
        
        # Apply uncertainty penalty if blocking
        if should_block:
            adjusted_score = min(adjusted_score, 50.0)  # Cap if too uncertain
        
        # Classify relativity type
        rel_type = self._classify_relativity_type(context)
        
        # Generate justification
        justification = self._generate_justification(
            base_harm_score,
            adjusted_score,
            context,
            rel_type,
            should_block
        )
        
        # Calculate dimension weights
        dimension_weights = self._calculate_dimension_weights(context)
        
        return RelativeEvaluation(
            base_harm_score=base_harm_score,
            adjusted_harm_score=adjusted_score,
            responsibility_multiplier=responsibility_mult,
            uncertainty_penalty=uncertainty_penalty,
            dimension_weights=dimension_weights,
            justification=justification,
            relatividad_type=rel_type,
            should_block=should_block
        )
    
    def _should_block_due_to_uncertainty(self, context: RelativeContext) -> bool:
        """
        Determina si la incertidumbre es tan alta que deber√≠a bloquear
        
        NEW LOGIC: Baja certeza ‚Üí Bloqueo, no aumento de da√±o
        """
        return context.epistemic_certainty < self.epistemic_block_threshold
    
    def _calculate_responsibility_multiplier(self, context: RelativeContext) -> float:
        """
        Calcula multiplicador de RESPONSABILIDAD (FIXED LOGIC)
        
        CAMBIOS CLAVE:
        - Alta capacidad ‚Üí AUMENTA responsabilidad (factor > 1.0)
        - Baja capacidad ‚Üí DISMINUYE responsabilidad (factor < 1.0)
        - Alta urgencia ‚Üí Justifica acci√≥n r√°pida
        - Varianza cultural ‚Üí Reduce certeza moral
        
        F√≥rmula corregida:
        - capacity_available = 0.9 ‚Üí factor ~1.4 (M√ÅS responsabilidad)
        - capacity_available = 0.1 ‚Üí factor ~0.6 (MENOS responsabilidad)
        """
        
        # === CAPACITY FACTOR (FIXED) ===
        # High capacity = high responsibility
        if context.capacity_available > 0.7:
            capacity_factor = 1.0 + (context.capacity_available - 0.7) * 1.5  # Up to 1.45x
        elif context.capacity_available < 0.3:
            capacity_factor = 0.5 + (context.capacity_available / 0.3) * 0.5  # Down to 0.5x
        else:
            capacity_factor = 1.0  # Neutral
        
        # === URGENCY FACTOR ===
        # High urgency = justifies quick action (slightly reduces weight of deliberation)
        if context.urgency_factor > 0.8:
            urgency_factor = 0.9  # Urgent situations reduce deliberation time
        elif context.urgency_factor < 0.2:
            urgency_factor = 1.1  # Non-urgent allows more careful analysis
        else:
            urgency_factor = 1.0
        
        # === TEMPORAL FACTOR ===
        # High temporal weight = time pressure
        temporal_factor = 1.0 - (context.temporal_weight * 0.1)
        
        # === CULTURAL FACTOR ===
        # High cultural variance = less universal certainty
        cultural_factor = 1.0 + (context.cultural_variance * 0.2)
        
        # Combine factors
        multiplier = (
            capacity_factor *
            urgency_factor *
            temporal_factor *
            cultural_factor
        )
        
        # Expanded range: [0.3, 3.0] for extreme cases
        return max(0.3, min(3.0, multiplier))
    
    def _calculate_uncertainty_penalty(self, context: RelativeContext) -> float:
        """
        Calcula penalizaci√≥n por incertidumbre epist√©mica
        
        NEW: Separado del multiplicador principal
        - Baja certeza ‚Üí Alta penalizaci√≥n
        - No aumenta el da√±o, aumenta la precauci√≥n
        """
        if context.epistemic_certainty < 0.3:
            return 0.7  # Heavy penalty
        elif context.epistemic_certainty < 0.5:
            return 0.85  # Moderate penalty
        else:
            return 1.0  # No penalty
    
    def _classify_relativity_type(self, context: RelativeContext) -> str:
        """Clasifica el tipo de relatividad predominante"""
        
        factors = {
            "CAPACITY": context.capacity_available,
            "EPISTEMIC": context.epistemic_certainty,
            "URGENCY": context.urgency_factor,
            "TEMPORAL": context.temporal_weight,
            "CULTURAL": context.cultural_variance
        }
        
        # Find dominant factor (furthest from neutral 0.5)
        dominant = max(factors.items(), key=lambda x: abs(x[1] - 0.5))
        
        if dominant[1] < 0.3:
            return f"LOW_{dominant[0]}"
        elif dominant[1] > 0.7:
            return f"HIGH_{dominant[0]}"
        else:
            return "BALANCED_CONTEXT"
    
    def _calculate_dimension_weights(
        self, 
        context: RelativeContext
    ) -> Dict[str, float]:
        """Calcula peso efectivo de cada dimensi√≥n"""
        
        weights = {
            "temporal": context.temporal_weight * self.dimension_base_weights[ContextDimension.TEMPORAL],
            "epistemic": context.epistemic_certainty * self.dimension_base_weights[ContextDimension.EPISTEMIC],
            "capacity": context.capacity_available * self.dimension_base_weights[ContextDimension.CAPACITY],
            "cultural": context.cultural_variance * self.dimension_base_weights[ContextDimension.CULTURAL],
            "urgency": context.urgency_factor * self.dimension_base_weights[ContextDimension.URGENCY]
        }
        
        # Normalize to sum = 1.0
        total = sum(weights.values())
        if total > 0:
            weights = {k: v/total for k, v in weights.items()}
        
        return weights
    
    def _generate_justification(
        self,
        base_score: float,
        adjusted_score: float,
        context: RelativeContext,
        rel_type: str,
        should_block: bool
    ) -> str:
        """Genera justificaci√≥n legible del ajuste (ENHANCED)"""
        
        delta = adjusted_score - base_score
        direction = "increased" if delta > 0 else "decreased"
        
        justification = f"""
üåê RELATIVE EVALUATION (FIXED LOGIC)

Base Score: {base_score:.1f}
Adjusted Score: {adjusted_score:.1f}
Change: {direction} {abs(delta):.1f} points

Relativity Type: {rel_type}

Context Factors:
‚Ä¢ Available Capacity: {context.capacity_available*100:.0f}%
‚Ä¢ Epistemic Certainty: {context.epistemic_certainty*100:.0f}%
‚Ä¢ Urgency Factor: {context.urgency_factor*100:.0f}%
‚Ä¢ Temporal Weight: {context.temporal_weight*100:.0f}%
‚Ä¢ Cultural Variance: {context.cultural_variance*100:.0f}%

Interpretation:
"""
        
        # Add specific interpretations
        if context.capacity_available > 0.7:
            justification += "\n‚úì HIGH CAPACITY ‚Üí Increased moral responsibility"
        elif context.capacity_available < 0.3:
            justification += "\n‚ö†Ô∏è LIMITED CAPACITY ‚Üí Reduced moral responsibility"
        
        if should_block:
            justification += "\nüö´ EPISTEMIC BLOCK: Uncertainty too high for authorization"
        elif context.epistemic_certainty < 0.5:
            justification += "\n‚ö†Ô∏è LOW CERTAINTY ‚Üí Requires additional precaution"
        
        if context.urgency_factor > 0.7:
            justification += "\n‚è∞ HIGH URGENCY ‚Üí Justifies rapid decision-making"
        
        if context.cultural_variance > 0.6:
            justification += "\nüåç HIGH CULTURAL VARIANCE ‚Üí Requires contextual sensitivity"
        
        return justification.strip()

    def integrate_with_divine_lock(
        self,
        divine_lock_state: str,
        guilt_score: float,
        context: RelativeContext
    ) -> Dict:
        """
        Integra evaluaci√≥n relativa con Divine Lock
        
        Returns:
            Dict con estado ajustado por relatividad
        """
        
        # Evaluate with context
        evaluation = self.evaluate_with_context(
            base_harm_score=guilt_score,
            context=context,
            scenario_description=""
        )
        
        # Determine if context justifies state change
        state_adjustment = self._should_adjust_state(
            current_state=divine_lock_state,
            evaluation=evaluation
        )
        
        return {
            "original_state": divine_lock_state,
            "adjusted_state": state_adjustment["new_state"],
            "state_changed": state_adjustment["changed"],
            "evaluation": evaluation,
            "context_summary": self._summarize_context(context),
            "should_block": evaluation.should_block
        }
    
    def _should_adjust_state(
        self,
        current_state: str,
        evaluation: RelativeEvaluation
    ) -> Dict:
        """Determina si el contexto justifica cambio de estado (ENHANCED)"""
        
        # If epistemic block, escalate to higher restriction
        if evaluation.should_block:
            if current_state in ["STABLE", "NOBLE_MODAL"]:
                return {"changed": True, "new_state": "RISK"}
            # Already restricted states stay restricted
        
        # Check responsibility multiplier
        if evaluation.responsibility_multiplier < 0.7:
            # Low responsibility ‚Üí reduce severity
            if current_state == "INFAMY":
                return {"changed": True, "new_state": "RISK"}
            elif current_state == "TOTAL_INFAMY":
                return {"changed": True, "new_state": "INFAMY"}
        
        elif evaluation.responsibility_multiplier > 1.5:
            # High responsibility ‚Üí increase severity
            if current_state == "RISK":
                return {"changed": True, "new_state": "INFAMY"}
            elif current_state == "STABLE":
                return {"changed": True, "new_state": "RISK"}
        
        return {"changed": False, "new_state": current_state}
    
    def _summarize_context(self, context: RelativeContext) -> str:
        """Genera resumen legible del contexto"""
        
        summary = []
        
        if context.capacity_available < 0.3:
            summary.append("üîª Very limited capacity")
        elif context.capacity_available > 0.7:
            summary.append("üî∫ High capacity available")
        
        if context.epistemic_certainty < 0.4:
            summary.append("‚ùì Epistemic uncertainty")
        elif context.epistemic_certainty > 0.8:
            summary.append("‚úì High certainty")
        
        if context.urgency_factor > 0.7:
            summary.append("‚è∞ Urgent situation")
        
        if context.cultural_variance > 0.6:
            summary.append("üåç Culturally sensitive context")
        
        return " | ".join(summary) if summary else "Standard context"


# ==========================================
# TESTING EXAMPLES
# ==========================================

if __name__ == "__main__":
    
    engine = RelativityEngine()
    
    # Test 1: High capacity = High responsibility (FIXED)
    print("=" * 80)
    print("TEST 1: HIGH CAPACITY ‚Üí INCREASED RESPONSIBILITY")
    print("=" * 80)
    
    context_high_cap = RelativeContext(
        temporal_weight=0.5,
        epistemic_certainty=0.9,  # Very certain
        capacity_available=0.95,  # VERY HIGH capacity
        cultural_variance=0.1,
        urgency_factor=0.5
    )
    
    eval1 = engine.evaluate_with_context(
        base_harm_score=50.0,
        context=context_high_cap,
        scenario_description="Agent with high capacity"
    )
    
    print(f"Base Score: 50.0")
    print(f"Adjusted Score: {eval1.adjusted_harm_score:.1f}")
    print(f"Responsibility Multiplier: {eval1.responsibility_multiplier:.2f}")
    print(f"Expected: > 1.0 (increased responsibility)")
    print(f"‚úì PASS" if eval1.responsibility_multiplier > 1.0 else "‚úó FAIL")
    
    # Test 2: Low epistemic certainty = BLOCK
    print("\n" + "=" * 80)
    print("TEST 2: LOW CERTAINTY ‚Üí EPISTEMIC BLOCK")
    print("=" * 80)
    
    context_low_cert = RelativeContext(
        temporal_weight=0.5,
        epistemic_certainty=0.2,  # Very uncertain
        capacity_available=0.8,
        cultural_variance=0.1,
        urgency_factor=0.5
    )
    
    eval2 = engine.evaluate_with_context(
        base_harm_score=70.0,
        context=context_low_cert,
        scenario_description="Uncertain situation"
    )
    
    print(f"Epistemic Certainty: {context_low_cert.epistemic_certainty}")
    print(f"Should Block: {eval2.should_block}")
    print(f"Expected: True")
    print(f"‚úì PASS" if eval2.should_block else "‚úó FAIL")
    
    # Test 3: Low capacity = Reduced responsibility
    print("\n" + "=" * 80)
    print("TEST 3: LOW CAPACITY ‚Üí REDUCED RESPONSIBILITY")
    print("=" * 80)
    
    context_low_cap = RelativeContext(
        temporal_weight=0.5,
        epistemic_certainty=0.8,
        capacity_available=0.15,  # Very low capacity
        cultural_variance=0.1,
        urgency_factor=0.5
    )
    
    eval3 = engine.evaluate_with_context(
        base_harm_score=50.0,
        context=context_low_cap,
        scenario_description="Agent with limited capacity"
    )
    
    print(f"Base Score: 50.0")
    print(f"Adjusted Score: {eval3.adjusted_harm_score:.1f}")
    print(f"Responsibility Multiplier: {eval3.responsibility_multiplier:.2f}")
    print(f"Expected: < 1.0 (reduced responsibility)")
    print(f"‚úì PASS" if eval3.responsibility_multiplier < 1.0 else "‚úó FAIL")
    
    print("\n" + "=" * 80)
    print("‚úÖ ALL TESTS COMPLETED - Check logic above")
    print("=" * 80)
